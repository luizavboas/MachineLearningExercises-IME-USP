{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício Proposto na disciplina de ML"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Porposition\n",
    "\n",
    "We must:\n",
    "\n",
    " - Simulate the difference between Error in and Error out from a (pre-established) universe and a well-defined and finite set of hypotheses.\n",
    " - Check the validity of the theorem: $P[|E_{in}(g)-E_{out}(g)|> \\epsilon]\\leq 2 M e^{-2\\epsilon ^2N}$, that assures us that $E_{in}(g) \\approx E_{out}(g)$ so we can use $E_{in}(g)$ as a proxy for $E_{out}(g)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_scatter_plot (X, y):\n",
    "    import numpy as np\n",
    "    from matplotlib import pyplot as plt\n",
    "    X=np.asarray(X)\n",
    "    y=np.asarray(y)\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.axis('equal')  #<-- set the axes to the same scale\n",
    "    plt.scatter(X[y==-1, 0], X[y==-1,1], c='red') \n",
    "    plt.scatter(X[y==1, 0], X[y==1,1], c='blue') \n",
    "    plt.title (\"Data Points\")\n",
    "    plt.xlabel (r\"$x_1$\")\n",
    "    plt.ylabel (r\"$x_2$\")\n",
    "    plt.grid(True)\n",
    "    plt.legend(['Negative Class', 'Positive Class'],loc='upper left')\n",
    "\n",
    "    ax = plt.gca()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_vector (origin,w, ax):\n",
    "    # u = origin\n",
    "    # w = vector\n",
    "    u = np.array(origin)\n",
    "    w = np.array(w)\n",
    "    v = w+u\n",
    "    ax.annotate('',xy=v,xytext=u,arrowprops={\"width\":0.8,\"headwidth\":5,'headlength':7, 'color':'black'})\n",
    "    ax.plot (v[0], v[1], \"ow\")\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_linear (w, b, ax, style=None):\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "\n",
    "    if style == None:\n",
    "        style = \"-k\"\n",
    "\n",
    "    w = np.array(w)\n",
    "    x_axis = np.linspace(xlim[0], xlim[1],100)\n",
    "    y_axis = (-b - w[0]*x_axis) / w[1]\n",
    "    y_lst = []\n",
    "    x_lst = []\n",
    "    \n",
    "    for i,y in enumerate(y_axis):\n",
    "        if ylim[0] <= y <= ylim[1]:\n",
    "            y_lst.append(y)\n",
    "            x_lst.append(x_axis[i])\n",
    "        if len (x_lst) > 0:\n",
    "            ax.plot(x_lst,y_lst, style)\n",
    "        \n",
    "        #return x_lst[-1], y_lst[-1]\n",
    "    return None, None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Standardization \n",
    "\n",
    "The preprocessing of data in ML, usually involves standardizing the data. That is, perform a linear transformation so that the center (mean value) is zero and the standard deviation is 1.\n",
    "\n",
    "The Universe must be standardized as well as the sample (resulting from sampling() function that takes the universe as paramater), even if this has already been done with the Universe, since the mean and standard deviation of the sample may not be the same values of the Universe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1683407631367,
     "user": {
      "displayName": "Luiza Vilas Boas",
      "userId": "01424730497908446706"
     },
     "user_tz": 180
    },
    "id": "LgJ6BFzkmLW2"
   },
   "outputs": [],
   "source": [
    "def standardization(X):\n",
    "    '''\n",
    "        Given a np.array X returns X_std: mean = 0, \n",
    "        std = 1 (not inplace - pure function)\n",
    "    '''\n",
    "    import numpy as np\n",
    "    X = np.asarray(X)\n",
    "    \n",
    "    #X_std = np.empty(X.shape)\n",
    "    #X_std[:,0] = (X[:,0]-np.mean(X[:,0]))/np.std(X[:,0])\n",
    "    #X_std[:,1] = (X[:,1]-np.mean(X[:,1]))/np.std(X[:,1])\n",
    "    \n",
    "    return  (X - X.mean(axis=0))/ X.std(axis=0)\n",
    "\n",
    "    return X_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 289,
     "status": "ok",
     "timestamp": 1683407773275,
     "user": {
      "displayName": "Luiza Vilas Boas",
      "userId": "01424730497908446706"
     },
     "user_tz": 180
    },
    "id": "9wIULLPVtjKh",
    "outputId": "49dfb9dd-53dc-41bc-beb6-7b01d47071c0"
   },
   "outputs": [],
   "source": [
    "#Testing Standarization\n",
    "import numpy as np\n",
    "\n",
    "X = np.asarray([[0,0],[1,0],\n",
    " [3,1],[3,2],[4,4]])\n",
    "X1=X[:,0]\n",
    "X2=X[:,1]\n",
    "X_std = standardization(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 448
    },
    "executionInfo": {
     "elapsed": 329,
     "status": "ok",
     "timestamp": 1683407649182,
     "user": {
      "displayName": "Luiza Vilas Boas",
      "userId": "01424730497908446706"
     },
     "user_tz": 180
    },
    "id": "6TPpvl6mtu0m",
    "outputId": "9ae25c72-4be0-486d-d541-9e4f9a76db67"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.axis('equal')  #<-- set the axes to the same scale\n",
    "plt.scatter(X[:,0],X[:,1])\n",
    "plt.title (\"Data Points\")\n",
    "plt.xlabel (r\"$x_1$\")\n",
    "plt.ylabel (r\"$x_2$\")\n",
    "plt.grid(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 448
    },
    "executionInfo": {
     "elapsed": 591,
     "status": "ok",
     "timestamp": 1683407651549,
     "user": {
      "displayName": "Luiza Vilas Boas",
      "userId": "01424730497908446706"
     },
     "user_tz": 180
    },
    "id": "QUPFX3FytwZP",
    "outputId": "ad1fe4ac-48df-4841-ea2b-616758ff037e"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.axis('equal')  #<-- set the axes to the same scale\n",
    "plt.scatter(X_std[:,0],X_std[:,1])\n",
    "plt.title (\"Data Points\")\n",
    "plt.xlabel (r\"$x_1$\")\n",
    "plt.ylabel (r\"$x_2$\")\n",
    "plt.grid(True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Error calculation (calc_error)\n",
    "\n",
    "This funciton will be useful to calculate $E_{in}$ and also $E_{out}$.\n",
    "The error must be normalized by the number of elements (be it the Universe or the sample sizes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 242,
     "status": "ok",
     "timestamp": 1683409039310,
     "user": {
      "displayName": "Luiza Vilas Boas",
      "userId": "01424730497908446706"
     },
     "user_tz": 180
    },
    "id": "cnuzaxYS71NM"
   },
   "outputs": [],
   "source": [
    "def calc_error(Y,Y_hat):\n",
    "    '''\n",
    "        Given Y (labels) and Y (predicts) returns normalized error\n",
    "\n",
    "        Inputs:\n",
    "        Y: np.array or list\n",
    "        Y_hat: idem\n",
    "    '''\n",
    "    import numpy as np\n",
    "\n",
    "    # Type fitting    \n",
    "    Y=np.asarray(Y)\n",
    "    Y_hat=np.asarray(Y_hat)\n",
    "        \n",
    "    #error = np.abs(Y_hat[np.abs(Y-Y_hat)>0])                            # error<-masked array of Y_hat\n",
    "    #norm_error = (np.sum(np.abs(np.abs(Y-Y_hat)>0)))/len(Y)             # normalized error\n",
    " \n",
    "    return (np.sum(np.abs(np.abs(Y-Y_hat)>0)))/len(Y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1683409039594,
     "user": {
      "displayName": "Luiza Vilas Boas",
      "userId": "01424730497908446706"
     },
     "user_tz": 180
    },
    "id": "lVPdzdKF9hDV"
   },
   "outputs": [],
   "source": [
    "a = np.zeros(5)\n",
    "b = np.ones(5)\n",
    "Y = [1,1,1,-1,-1] \n",
    "Y_hat = [-1,1,-1,1,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1683409039866,
     "user": {
      "displayName": "Luiza Vilas Boas",
      "userId": "01424730497908446706"
     },
     "user_tz": 180
    },
    "id": "lXs4UTgb98mI",
    "outputId": "d0a340f6-1f90-49d9-a495-53a2a0a526d1"
   },
   "outputs": [],
   "source": [
    "calc_error(a,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1683409040136,
     "user": {
      "displayName": "Luiza Vilas Boas",
      "userId": "01424730497908446706"
     },
     "user_tz": 180
    },
    "id": "9kJyJkoy998r",
    "outputId": "0c528708-cc1a-47fa-d154-a112dd9f781f"
   },
   "outputs": [],
   "source": [
    "calc_error(Y,Y_hat)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Sampling\n",
    "\n",
    "Sampling will take a N-size sample from the Universe, defined by de X array and its labels stored in Y. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1683409041534,
     "user": {
      "displayName": "Luiza Vilas Boas",
      "userId": "01424730497908446706"
     },
     "user_tz": 180
    },
    "id": "ui7_mnuI-Axk"
   },
   "outputs": [],
   "source": [
    "def sampling(N,X,Y,random_state=42):\n",
    "    '''\n",
    "        Given the arguments:\n",
    "          - N: #of samples to be taken from (X,Y) : int,\n",
    "        \n",
    "          (X,Y): Universe\n",
    "          - X: points in R² : np.array and\n",
    "          - Y: labels for X: np.array {1,0}\n",
    "        \n",
    "        Returns:\n",
    "          - sample_N: sample of N elements of (X,Y) : np.array \n",
    "    '''\n",
    "    from numpy.random import RandomState\n",
    "    \n",
    "    # Type fitting\n",
    "    X = np.asarray(X)\n",
    "    \n",
    "    j,k = X.shape\n",
    "    \n",
    "    if not j==len(Y):\n",
    "        raise TypeError(\"X and Y must have the same number of lines\") \n",
    "        \n",
    "    # Defining the N-size random index of (X,Y)\n",
    "    #rand_index = rs.randint(0, high=len(Y), size=N, dtype=int)\n",
    "    rand_index = np.random.randint(0, high=len(Y), size=N, dtype='l')\n",
    "    \n",
    "     # Type fitting\n",
    "    Y = np.asarray(Y)\n",
    "    \n",
    "    # Sample array with N lines and k+1 columns\n",
    "    # X_sample = np.empty((N,k))\n",
    "    # Y_sample = np.empty((N,1))\n",
    "    \n",
    "    # for i in range(0,N,1):\n",
    "    #    X_sample[i,:] = X[rand_index[i],:]\n",
    "    #    Y_sample[i,0] = Y[rand_index[i]]\n",
    "    \n",
    "    # Standarization of the sample\n",
    "    # X_sample = standardization(X_sample)\n",
    "  \n",
    "    return (X[rand_index,:], Y[rand_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing smapling()\n",
    "\n",
    "X = np.asarray([[0,0],[1,0],\n",
    "[3,1],[3,2],[4,4]])\n",
    "\n",
    "X_sample,y_sample=sampling(3,X,Y,42)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Diagonals\n",
    "\n",
    "Function of Diagonals: 45º straight lines (angular coefficient +1 and -1, varying 'bias' forwards and backwards - 'bias' step (b passed as a parameter) defined by the range $[-M/4,M/4]$)\n",
    "\n",
    "We know that:\n",
    "- $X_0 * w[0] + X1 * w[1] +bias = 0$ and\n",
    "- $w = [1,1]$, represents the case of the line with negative slope and\n",
    "- $w = [1,-1]$, the case of a line with a positive slope\n",
    "\n",
    "The following order must be used:\n",
    "bias from $-(M/4)*b$ to $(M/4)*b$ (exclusive).\n",
    "\n",
    "The line with negative slope (coef == -1), vector $w = [1,1]$ (perpendicular to the straight line), and bias is calculated first and then the line with positive slope,vector $w = [1,-1]$, and the same bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diagonais(X,M,b): # valor:2.5\n",
    "  '''\n",
    "    Função Diagonais: retas 45º (coeficiente angular +1 e -1  variando bias \n",
    "    um tanto para frente e um tanto para trás - passo do bias (b passado por parâmetro) \n",
    "    definido pelo intervalo [-M//4,M//4)\n",
    "\n",
    "    Sabendo que: \n",
    "      x0 * w[0] + x1 * w[1] + bias = 0 e que\n",
    "      w = [1,1] no caso da reta com inclinação negativa e\n",
    "      w = [1,-1] no caso da reta com inclinação positiva\n",
    "\n",
    "    A seguinte ordem deve ser utilizada:\n",
    "      bias partindo de -(M//4) * b até (M//4) * b (exclusive)\n",
    "      A reta com inclinação negativa (coef == -1), vetor w = [1,1] (perpendicular a reta), e bias é calculda primeiro \n",
    "      e a na sequência reta com inclinação positiva, vetor w = [1,-1], e o mesmo bias.\n",
    "      Conforme mostrado nos plots!\n",
    "\n",
    "\tparâmetros:\n",
    "\t\tX: np.array\n",
    "\t\tM: número de hipóteses do universo (número inteiro) - espera-se um múltiplo de 4\n",
    "\tRetorna \n",
    "\t\tpredict: np.array de np.array de y_hat, um y_hat para cada hipótese (reta), deve ter tamanho M\n",
    "   '''\n",
    "  import numpy as np\n",
    "  X = np.asarray(X)\n",
    "  \n",
    "  j,k = X.shape\n",
    "\n",
    "  # Initial bias value\n",
    "  bias = -(M/4)*b\n",
    "\n",
    "  predicts = np.empty((M,j), dtype = 'l')\n",
    "  i=0       # predicts index\n",
    "    \n",
    "    \n",
    "  while bias<(M/4)*b:\n",
    "    \n",
    "      # Negative slope\n",
    "      w = [1,1]\n",
    "      predicts[i,:] = np.sign(X[:,0]*w[0] + X[:,1]*w[1] + bias)\n",
    "      i+=1\n",
    "      \n",
    "      # Positive slope\n",
    "      w = [1,-1]\n",
    "      predicts[i,:] = np.sign(X[:,0]*w[0] + X[:,1]*w[1] + bias)\n",
    "      i+=1\n",
    "      \n",
    "      bias = bias+1\n",
    " \n",
    "  predicts[predicts==0]=1\n",
    "  \n",
    "  return predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing 1 diagonals\n",
    "\n",
    "M=4\n",
    "b=1\n",
    "\n",
    "predicts=diagonais(X_std,4,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "b = -(M/4)*b\n",
    "\n",
    "for i in range(0,M):\n",
    "    #i=1\n",
    "    print(\"bias:\\n\",b)\n",
    "    ax = draw_scatter_plot (X_std, predicts[i,:])\n",
    "\n",
    "    \n",
    "    if (i%2==0):\n",
    "        w = [1, 1]\n",
    "        draw_linear (w, b, ax)\n",
    "        \n",
    "    else:\n",
    "        w = [1, -1]        \n",
    "        draw_linear (w, b, ax)\n",
    "        b+=1\n",
    "    \n",
    "\n",
    "    # Pick up a point on the decision boundary\n",
    "    u = np.array([0.,0.])\n",
    "    u[0] = 0\n",
    "    u[1] = 0\n",
    "    #u[1] = (-b - w[0]*u[0]) / w[1]\n",
    "    draw_vector (u, w, ax) # vector w at a point in the hyperplane\n",
    "    \n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Euclidean Distance (euclidean_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_dist(p,q): # valor:0.5\n",
    "  '''\n",
    "    Given two points (np.array) returns the euclidean distance between them\n",
    "  '''\n",
    "  import numpy as np\n",
    "  \n",
    "  # Type fitting\n",
    "  p=np.asarray(p)\n",
    "  q=np.asarray(q)\n",
    "   \n",
    " # dist = (np.sum(np.power((p-q),2)))**(1/2)\n",
    "  dist = np.power(np.sum(np.power((p-q),2)),1/2)\n",
    "\n",
    "  return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "euclidean_dist\n",
    "P = [6.0, 3.0]\n",
    "Q = [3.0, 7.0]\n",
    "print(euclidean_dist(P, Q))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Egocentric "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def egocentric(X,C,r): # valor:2.0\n",
    "  '''\n",
    "    Given a dataset X (np.array), C (np.array) are the points that will be used as centers, and a radius r: \n",
    "      For each point in C, Creates a circumference c, each center works as an hypothesis, and classify points inside c as +1\n",
    "      otherwise -1.\n",
    "      Returns all predicts (an list for each point (used as center) )\n",
    "  '''\n",
    "  # Type fitting\n",
    "  X = np.asarray(X)\n",
    "  C = np.asarray(C)\n",
    "  r = np.float(np.abs(r))\n",
    "  \n",
    "  j,k = X.shape\n",
    "  cj,ck = C.shape\n",
    "  \n",
    "  # Number of rows = # of hypothesis = # of points in C array\n",
    "  # Number of columns in 'predicts' = # labels for 'X' (each point has 1 label {+1,-1})\n",
    "   \n",
    "  predicts = np.empty((cj,j), dtype = 'l')\n",
    "  \n",
    "    \n",
    "  for i in np.arange(0,cj,1):\n",
    "      for p in  np.arange(0,j,1):\n",
    "          dist = euclidean_dist(C[i],X[p])\n",
    "          predicts[i,p] = np.abs(dist<r)\n",
    "    \n",
    "    \n",
    "  \n",
    "  predicts = np.asarray(predicts)\n",
    "  predicts[predicts==0]=-1    \n",
    "  \n",
    "  return predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r=1.1478385915666818"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.asarray([[0,0],[1,0],[3,1],[3,2],[4,4]])\n",
    "Y = np.asarray([-1,-1,1,1,1])\n",
    "N = 4\n",
    "\n",
    "X_std = standardization(X)\n",
    "X_sampled_std,Y_sampled = sampling(N,X_std,Y,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "egocentric(X_std,X_sampled_std,1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Frequency Calculation (calc_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_freq(N,H_set,eps,X,Y,M=100,b=0.05,r=1,random_state = 42): # valor:3.0\n",
    "  '''\n",
    "  Given N # of samples(integer), H_set name of the hypotheses set \n",
    "  (string <diagonais> or <egocentric> error will be returned otherwise)\n",
    "  eps: epsilon (abs(error_in - error_out) desired), X from the Universe data (np.array - complete dataset),\n",
    "  Y is all label from theentire Universe(np.array), M # of hypotheses used if <diagonais> is chosen, \n",
    "  B: is the bias used when <diagonais> is chosen, r radius of the circumference if <egocentric> is chosen, \n",
    "  random_state to set the seed\n",
    "\n",
    "  Returns:\n",
    "    bound: theoretical bound for Pr[abs(error_in - error_out) > eps]\n",
    "    probs: approximated probability of Pr[abs(error_in - error_out) <= eps] by the frequency \n",
    "      (# of occurancies (abs(error_in - error_out) <= eps) / # of hipotheses)\n",
    "  '''\n",
    "  import re \n",
    "  import numpy as np\n",
    "\n",
    "  # Type fitting    \n",
    "  Y = np.asarray(Y)\n",
    "  X = np.asarray(X)\n",
    "    \n",
    "  # Sampling\n",
    "  X_std = standardization(X)\n",
    "  X_sampled,Y_sampled = sampling(N,X,Y,random_state)\n",
    "  X_sampled_std = standardization(X_sampled)\n",
    "\n",
    "  Y_sampled[Y_sampled==0]=-1\n",
    "  \n",
    "  # Generating the hypothesis set\n",
    "  if re.match('diagonais', H_set, flags=re.IGNORECASE):\n",
    "      Y_hat = diagonais(X_sampled_std,M,b)\n",
    "      H_set_size = M\n",
    "      Y_test = diagonais(X_std,M,b)\n",
    "        \n",
    "  elif re.match('egocentric', H_set, flags=re.IGNORECASE):\n",
    "      Y_hat = egocentric(X_sampled_std,X_sampled_std,r)\n",
    "      H_set_size = N\n",
    "      Y_test = egocentric(X_std,X_sampled_std,r)\n",
    "  \n",
    "    \n",
    "  # Error arrays \n",
    "  error_in = np.zeros((1,H_set_size), dtype=np.float64)\n",
    "  error_out = np.zeros((1,H_set_size), dtype=np.float64)\n",
    "    \n",
    "  # Sacanning the entire hypothesis set to calculate the errors\n",
    "  for i in np.arange(0,H_set_size,1): \n",
    "      error_in[:,i] = calc_error(Y_sampled, Y_hat[i,:])\n",
    "      error_out[:,i] = calc_error(Y, Y_test[i,:])\n",
    "    \n",
    "    \n",
    "  # Theoretical bound for P[|E_in-E_out|>eps]\n",
    "  bound = 2*H_set_size*np.exp(-2*(eps**2)*N)\n",
    "    \n",
    "  # Array of diferences between E_in and E_out for each hypothesis\n",
    "  difference = np.abs(error_out-error_in)\n",
    "  \n",
    "  print(np.abs(difference[difference<=eps]))\n",
    "  \n",
    "  difference[difference<=eps] = np.ma.masked\n",
    "    \n",
    "  #  Pr[abs(error_in - error_out) <= eps]\n",
    "  probs = np.sum(np.ma.count(difference))/H_set_size\n",
    " \n",
    "    \n",
    "  return (bound,probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "X,Y = make_classification(n_samples = 3000, \n",
    "                          n_classes = 2,\n",
    "                          n_features = 2, \n",
    "                          n_redundant = 0,\n",
    "                          n_informative = 2, \n",
    "                          n_clusters_per_class = 1,\n",
    "                          n_repeated = 0, hypercube = True,\n",
    "                          weights = [0.3,],\n",
    "                          random_state = seed)\n",
    "eps = 0.1 \n",
    "N = 700\n",
    "M = 500\n",
    "b = 1\n",
    "\n",
    "Y[Y==0]=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "(bound,probs): (0.0008315287191035649, 1.0)\n"
     ]
    }
   ],
   "source": [
    "bound,probs = calc_freq(N,\"diagonais\",eps,X,Y,M,b)\n",
    "print(\"(bound,probs):\",(bound,probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luiza\\AppData\\Local\\Temp\\ipykernel_16448\\778761334.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  r = np.float(np.abs(r))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700\n"
     ]
    }
   ],
   "source": [
    "bound,probs = calc_freq(N,\"egocentric\",eps,X,Y,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(bound,probs): (0.0011641402067449908, 1.0)\n"
     ]
    }
   ],
   "source": [
    "print(\"(bound,probs):\",(bound,probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sampled,Y_sampled = sampling(N,X,Y)\n",
    "X_std = standardization(X_sampled)\n",
    "predicts = diagonais(X_std,M,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(700, 2)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Y_std' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\luiza\\OneDrive\\Documents\\02_JOBS\\Mestrado\\IME\\MAC5832\\Listas\\MAC5832_EP1.ipynb Cell 39\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/luiza/OneDrive/Documents/02_JOBS/Mestrado/IME/MAC5832/Listas/MAC5832_EP1.ipynb#X53sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(X_std\u001b[39m.\u001b[39mshape)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/luiza/OneDrive/Documents/02_JOBS/Mestrado/IME/MAC5832/Listas/MAC5832_EP1.ipynb#X53sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(Y_std\u001b[39m.\u001b[39mshape)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/luiza/OneDrive/Documents/02_JOBS/Mestrado/IME/MAC5832/Listas/MAC5832_EP1.ipynb#X53sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(predicts\u001b[39m.\u001b[39mshape)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Y_std' is not defined"
     ]
    }
   ],
   "source": [
    "print(X_std.shape)\n",
    "print(Y_std.shape)\n",
    "print(predicts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,M):\n",
    "    #i=1\n",
    "    print(\"bias:\\n\",b)\n",
    "    ax = draw_scatter_plot (X_std, predicts[i,:])\n",
    "\n",
    "    \n",
    "    if (i%2==0):\n",
    "        w = [1, 1]\n",
    "        draw_linear (w, b, ax)\n",
    "        \n",
    "    else:\n",
    "        w = [1, -1]        \n",
    "        draw_linear (w, b, ax)\n",
    "        b+=1\n",
    "    \n",
    "\n",
    "    # Pick up a point on the decision boundary\n",
    "    u = np.array([0.,0.])\n",
    "    u[0] = 0\n",
    "    u[1] = 0\n",
    "    #u[1] = (-b - w[0]*u[0]) / w[1]\n",
    "    draw_vector (u, w, ax) # vector w at a point in the hyperplane\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = draw_scatter_plot (X_std,Y_sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPqsDBq6nJIfQBIfNec09dr",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
